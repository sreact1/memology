{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение или просто добавь воды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def commitChanges():\n",
    "    ! git add KnowYourMemesParser.ipynb\n",
    "    ! git commit -m \"added function for full meme page parsing\"\n",
    "    ! git push -u origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def commitPictures():\n",
    "    ! git add pictures/\n",
    "    ! git commit -m \"added pictures\"\n",
    "    ! git push -u origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master e4229a4] added function for full meme page parsing\n",
      " 1 file changed, 190 insertions(+), 9 deletions(-)\n",
      "Counting objects: 3, done.\n",
      "Delta compression using up to 4 threads.\n",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects: 100% (3/3), 1.85 KiB | 0 bytes/s, done.\n",
      "Total 3 (delta 2), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
      "To https://github.com/DmitrySerg/memology.git\n",
      "   ad7ec12..e4229a4  master -> master\n",
      "Branch master set up to track remote branch master from origin.\n"
     ]
    }
   ],
   "source": [
    "commitChanges()\n",
    "#commitPictures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a surprise tool that will help us later!\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Наш первый запрос"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для наших благородных исследовательских целей нужно собрать данные по каждому мему с соответствующей ему страницы. Но для начала нужно получить адреса этих страниц. Поэтому открываем основную страницу со всеми выложенными мемами. Выглядит она следующим образом:\n",
    "\n",
    "<img align=\"center\" src=\"pictures/memes_main.png\" height=\"500\" width=\"500\"> \n",
    "\n",
    "Отсюда мы и будем тащить ссылки на каждый из перечисленных мемов. Сохраним в переменную `page_link` адрес основной страницы и откроем её при помощи библиотеки `requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page_link = 'http://knowyourmeme.com/memes/all/page/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [403]>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(page_link)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот и первая проблема! Обращаемся к [главному источнику знаний](https://en.wikipedia.org/wiki/HTTP_403) и выясняем, что 403-я ошибка выдается сервером, если он доступен и способен обрабатывать запросы, но по некоторым личным причинам отказывается это делать. Попробуем выяснить, почему. Для этого посмотрим, как выглядел финальный запрос, отправленный нами на сервер, а в первую очередь - как выглядел наш User-Agent в глазах сервера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Agent: python-requests/2.14.2\n",
      "Accept-Encoding: gzip, deflate\n",
      "Accept: */*\n",
      "Connection: keep-alive\n"
     ]
    }
   ],
   "source": [
    "for key, value in response.request.headers.items():\n",
    "    print(key+\": \"+value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Похоже, мы недвусмысленно дали понять серверу, что мы сидим на питоне и используем библиотеку requests под версией 2.14.2. Скорее всего, это вызвало у сервера некоторые подозрения относительно наших благих намерений и он решил нас безжалостно отвергнуть. Для сравнения, можно посмотреть, как выглядят request-headers у здорового человека:\n",
    "\n",
    "<img align=\"center\" src=\"pictures/good_headers.png\" height=\"800\" width=\"800\"> \n",
    "\n",
    "Очевидно, что нашему скромному запросу не тягаться с таким обилием мета-информации, которое передается при запросе из обычного браузера. К счастью, никто нам не мешает притвориться человечными и пустить пыль в глаза сервера при помощи генерации фейкового юзер-агента. Библиотек, которые справляются с такой задачей, существует очень и очень много, лично мне больше всего нравится [`fake-useragent`](https://pypi.python.org/pypi/fake-useragent). При вызове метода из различных кусочков будет генерироваться рандомное сочетание операционной системы, спецификаций и версии браузера, которые можно передавать в запрос:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UserAgent().chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(page_link, headers={'User-Agent': UserAgent().chrome})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Замечательно, наша небольшая маскировка сработала и обманутый сервер покорно выдал благословенный 200 ответ — соединение установлено и данные получены, всё чудесно! Посмотрим, что же все-таки мы получили."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html>\\n<html xmlns:fb=\\'http://www.facebook.com/2008/fbml\\' xmlns=\\'http://www.w3.org/1999/xhtml\\'>\\n<head>\\n<meta content=\\'text/html; charset=utf-8\\' http-equiv=\\'Content-Type\\'>\\n<script type=\"text/javascript\">window.NREUM||(NREUM={});NREUM.info={\"beacon\":\"bam.nr-data.net\",\"errorBeacon\":\"bam.nr-data.net\",\"licenseKey\":\"c1a6d52f38\",\"applicationID\":\"31165848\",\"transactionName\":\"dFdfRUpeWglTQB8GDUNKWFRLHlcJWg==\",\"queueTime\":0,\"applicationTime\":66,\"agent\":\"\"}</script>\\n<script type=\"text/javascript\">window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var o=n[t]={exports:{}};e[t][0].call(o.exports,function(n){var o=e[t][1][n];return r(o||n)},o,o.exports)}return n[t].exports}if(\"function\"==typeof __nr_require)return __nr_require;for(var o=0;o<t.length;o++)r(t[o]);return r}({1:[function(e,n,t){function r(){}function o(e,n,t){return function(){return i(e,[c.now()].concat(u(arguments)),n?null:this,t),n?void 0:this}}var i=e(\"handle\"),a=e(2),u=e(3),f=e(\"ee\").get(\"tracer\")'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит неудобоваримо, как насчет сварить из этого дела что-то покрасивее? Например, красивый суп."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Красивый суп\n",
    "\n",
    "<img align=\"center\" src=\"https://www.crummy.com/software/BeautifulSoup/10.1.jpg\" height=\"200\" width=\"200\"> \n",
    "\n",
    "Пакет **[bs4 , a.k.a BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/)** (тут есть гиперссылка на лучшего друга человека — документацию) был назван в честь стишка про красивый суп из Алисы в стране чудес.\n",
    "\n",
    "Красивый суп. Эта совершенно волшебная библиотека, которая из сырого и необработанного HTML (или XML) кода страницы выдаст вам структурированный массив данных, по которому очень удобно искать необходимые теги, классы, атрибуты, тексты и прочие элементы веб страниц.\n",
    "\n",
    "> Пакет под названием `BeautifulSoup` — скорее всего, не то, что вам нужно. Это третья версия (*Beautiful Soup 3*), а мы будем использовать четвертую. Так что нам нужен пакет `beautifulsoup4`. Чтобы было совсем весело, при импорте нужно указывать другое название пакета — `bs4`, а импортировать функцию под названием `BeautifulSoup`. В общем, сначала легко запутаться, но эти трудности нужно преодолеть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Передадим функции `BeautifulSoup` текст веб-страницы, которую мы недавно получили."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html,'html.parser') # В опции также можно указать lxml, \n",
    "                                         # если предварительно установить одноименный пакет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим что-то вот такое:\n",
    "    \n",
    "```\n",
    "<!DOCTYPE html>\n",
    "\n",
    "<html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:fb=\"http://www.facebook.com/2008/fbml\">\n",
    "<head>\n",
    "<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
    "<script type=\"text/javascript\">window.NREUM||(NREUM={});NREUM.info={\"beacon\":\"bam.nr-data.net\",\"errorBeacon\":\"bam.nr-data.net\",\"licenseKey\":\"c1a6d52f38\",\"applicationID\":\"31165848\",\"transactionName\":\"dFdfRUpeWglTQB8GDUNKWFRLHkUNWUU=\",\"queueTime\":0,\"applicationTime\":24,\"agent\":\"\"}</script>\n",
    "<script type=\"text/javascript\">window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var o=n[t]={exports:{}};e[t][0].call(o.exports,function(n){var o=e[t][1][n];return r(o||n)},o,o.exports)}return n[t].exports}if(\"function\"==typeof __nr_require)return __nr_require;for(var o=0;o<t.length;o++)r(t[o]);return r}({1:[function(e,n,t){function r(){}function o(e,n,t){return function(){return i(e,[c.now()].concat(u(arguments)),n?null:this,t),n?void 0:this}}var i=e(\"handle\"),a=e(2),u=e(3),f=e(\"ee\").get(\"tracer\"),c=e(\"loader\"),s=NREUM;\"undefined\"==typeof window.newrelic&&(newrelic=s);var p=\n",
    "```\n",
    "\n",
    "Стало намного лучше, не правда ли? Что же лежит в переменной `soup`? Невнимательный пользователь, скорее всего, скажет,что ничего вообще не изменилось. Тем не менее, это не так. Теперь мы можем свободно бродить по HTML-дереву страницы, искать детей, родителей и вытаскивать их! \n",
    "\n",
    "Например, можно бродить по вершинам, указывая путь из тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>All Entries | Know Your Meme</title>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.html.head.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно вытащить из того места, куда мы забрели, текст с помощью метода `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All Entries | Know Your Meme'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.html.head.title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более того, зная адрес элемента, мы сразу можем найти его. Например, можно сделать это по классу. Следующая команда должна найти элемент, который лежит внутри тега `a` и имеет класс `photo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"photo left\" href=\"/memes/events/al-franken-sexual-misconduct-allegations\" target=\"_self\"><img alt=\"Al Franken Accused Of Sexual Misconduct\" data-src=\"http://i0.kym-cdn.com/featured_items/icons/wide/000/007/402/Screen_Shot_2017-11-16_at_2.00.18_PM.png\" height=\"112\" src=\"http://a.kym-cdn.com/assets/blank-b3f96f160b75b1b49b426754ba188fe8.gif\" title=\"Al Franken Accused Of Sexual Misconduct\" width=\"198\"/> <div class=\"info abs\"> <div class=\"c\"> Al Franken Accused Of Sexual Misconduct </div> </div> </a>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = soup.find('a', attrs = {'class':'photo'})\n",
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, вопреки нашим ожиданиям, вытащенный объект имеет класс `\"photo left\"`. Оказывается, `BeautifulSoup4` расценивает аттрибуты `class` как набор отдельных значений, поэтому `\"photo left\"` для библиотеки равносильно `[\"photo\", \"left\"]`, а указанное нами значение этого класса `\"photo\"` входит в этот список. Чтобы избежать такой неприятной ситуации и проходов по ненужным нам ссылкам, придется воспользоваться собственной функцией и задать строгое соответствие:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"photo\" href=\"/memes/eas-new-handheld-console\"><img alt=\"EA's new handheld console\" data-src=\"http://i0.kym-cdn.com/entries/icons/medium/000/024/689/Capture.JPG\" src=\"http://a.kym-cdn.com/assets/blank-b3f96f160b75b1b49b426754ba188fe8.gif\" title=\"EA's new handheld console\"/> <div class=\"entry-labels\"> <span class=\"label label-deadpool\"> Deadpool </span> </div> </a>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = soup.find(lambda tag: tag.name == 'a' and tag.get('class') == ['photo'])\n",
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученный после поиска объект также обладает структурой bs4. Поэтому можно продолжить искать нужные нам объекты уже в нём! Вытащим ссылку на этот мем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/memes/eas-new-handheld-console'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.attrs['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что после всех этих безумных преобразований у данных поменялся тип. Теперь они `str`. Это означет, что с ними можно работать как с текстом и пускать в ход для отсеивания лишней информации регулярные выражения.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тип данных до вытаскивания ссылки: <class 'bs4.element.Tag'>\n",
      "Тип данных после вытаскивания ссылки: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Тип данных до вытаскивания ссылки:\", type(obj))\n",
    "print(\"Тип данных после вытаскивания ссылки:\", type(obj.attrs['href']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если несколько элементов на странице обладают указанным адресом, то метод `find` вернёт только самый первый.  Чтобы найти все элементы с таким адресом, нужно использовать метод `findAll`. На выход будет выдан список! Таким образом, мы можем получить одним поиском сразу все объекты, содержащие ссылки на страницы с мемами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"photo\" href=\"/memes/eas-new-handheld-console\"><img alt=\"EA's new handheld console\" data-src=\"http://i0.kym-cdn.com/entries/icons/medium/000/024/689/Capture.JPG\" src=\"http://a.kym-cdn.com/assets/blank-b3f96f160b75b1b49b426754ba188fe8.gif\" title=\"EA's new handheld console\"/> <div class=\"entry-labels\"> <span class=\"label label-deadpool\"> Deadpool </span> </div> </a>,\n",
       " <a class=\"photo\" href=\"/memes/7-days-7-photos\"><img alt=\"7 Days, 7 Photos\" data-src=\"http://i0.kym-cdn.com/entries/icons/medium/000/024/688/7days.jpg\" src=\"http://a.kym-cdn.com/assets/blank-b3f96f160b75b1b49b426754ba188fe8.gif\" title=\"7 Days, 7 Photos\"/> <div class=\"entry-labels\"> <span class=\"label label-submission\"> Submission </span> </div> </a>,\n",
       " <a class=\"photo\" href=\"/memes/pope-francis-signs-a-lamborghini\"><img alt=\"Pope Francis Signs a Lamborghini\" data-src=\"http://i0.kym-cdn.com/entries/icons/medium/000/024/687/pope.jpg\" src=\"http://a.kym-cdn.com/assets/blank-b3f96f160b75b1b49b426754ba188fe8.gif\" title=\"Pope Francis Signs a Lamborghini\"/> <div class=\"entry-labels\"> <span class=\"label label-submission\"> Submission </span> </div> </a>]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meme_links = soup.findAll(lambda tag: tag.name == 'a' and tag.get('class') == ['photo'])\n",
    "meme_links[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось очистить полученный список от мусора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meme_links = [link.attrs['href'] for link in meme_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/memes/eas-new-handheld-console',\n",
       " '/memes/7-days-7-photos',\n",
       " '/memes/pope-francis-signs-a-lamborghini',\n",
       " '/memes/subcultures/justice-league',\n",
       " '/memes/travis-scotts-concert-pic',\n",
       " '/memes/events/sylvester-stallone-sexual-assault-allegation',\n",
       " '/memes/dilly-dilly',\n",
       " '/memes/%F0%9F%91%89%F0%9F%98%8E%F0%9F%91%89-zoop',\n",
       " '/memes/follow-me-to',\n",
       " '/memes/events/fuck-trump-truck-decal']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meme_links[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(meme_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готово, получили ровно 16 ссылок по числу мемов на одной странице поиска. \n",
    "\n",
    "Окей, то, что можно искать элемент по его адресу, конечно же, круто, но откуда взять этот адрес? \n",
    "\n",
    "Можно установить для своего браузера какую-нибудь утилиту, позволяющую вытаскивать со страницы нужные теги. Например, [selectorgadget.](http://selectorgadget.com/)\n",
    "\n",
    "Тем не менее, это путь не истинного самурая. Для последователей бусидо есть другой способ — искать теги для каждого нужного нам элемента вручную. Для этого нам придётся жать правой кнопкой мышки по окошечку браузера и тыкать кнопку **Inspect**. Ваше окно браузера будет после всех этих манипуляций выглядеть как-то вот так: \n",
    "\n",
    "<img align=\"center\" src=\"pictures/memes_inspection.png\" height=\"800\" width=\"800\"> \n",
    "\n",
    "Если тыкнуть кнопочку, которая находится в огромном красном кружочке, а после навести курсорчик на требующийся вам кусочек странички и тыкнуть на него, то справа, там, где находится гигантская стрелочка, выскочит кусок html-кода, в котором находится адрес выбранного вами объекта. Этот адрес можно смело копировать в код и наслаждаться своей брутальностью. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, обернем в красивую функцию все-все манипуляции, проделанные выше:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPageLinks(page_number):\n",
    "    \"\"\"\n",
    "        Возвращает список ссылок на мемы, полученный с текущей страницы\n",
    "        \n",
    "        page_number: int/string\n",
    "            номер страницы для парсинга\n",
    "            \n",
    "    \"\"\"\n",
    "    # составляем ссылку на страницу поиска\n",
    "    page_link = 'http://knowyourmeme.com/memes/all/page/{}'.format(page_number)\n",
    "    \n",
    "    # запрашиваем данные по ней\n",
    "    response = requests.get(page_link, headers={'User-Agent': UserAgent().chrome})\n",
    "    \n",
    "    if not response.ok:\n",
    "        # если сервер нам отказал, вернем пустой лист для текущей страницы\n",
    "        return [] \n",
    "    \n",
    "    # получаем содержимое страницы и переводим в суп\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    # наконец, ищем ссылки на мемы и очищаем их от ненужных тэгов\n",
    "    meme_links = soup.findAll(lambda tag: tag.name == 'a' and tag.get('class') == ['photo'])\n",
    "    meme_links = ['http://knowyourmeme.com' + link.attrs['href'] for link in meme_links]\n",
    "    \n",
    "    return meme_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем функцию и убедимся, что всё хорошо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://knowyourmeme.com/memes/locked',\n",
       " 'http://knowyourmeme.com/memes/events/hollywood-sexual-harassment-allegations']"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meme_links = getPageLinks(1)\n",
    "meme_links[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://knowyourmeme.com/memes/i-killed-them-all',\n",
       " 'http://knowyourmeme.com/memes/events/al-franken-sexual-misconduct-allegations']"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meme_links = getPageLinks(2)\n",
    "meme_links[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, функция работает и теперь мы теоретически можем достать ссылки на все 17 171 мем, для чего нам придется пройтись по 17 171 / 16 ~ 1074 страницам. Прежде чем расстраивать сервер таким количеством запросов, посмотрим, как доставать всю необходимую информацию о конкретном меме. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Финальная подготовка к грабежу\n",
    "\n",
    "По аналогии со ссылками можно вытащить что угодно. Для этого надо сделать несколько шагов: \n",
    "\n",
    "1. Открываем страничку с мемом\n",
    "2. Находим тег для нужной нам информации\n",
    "3. Пихаем всё это в красивый суп\n",
    "4. ......\n",
    "5. Profit \n",
    "\n",
    "Для закрепления информации в голове любознательного читателя, вытащим число просмотров мема.\n",
    "\n",
    "А в качестве примера возьмем самый популярный на этом сайте мем - Doge, набравший более 12 миллионов просмотров по состоянию на 18 ноября 2017 года. \n",
    "\n",
    "Сама страница, с которой мы будем доставать дорогую нашему исследовательскому сердцу информацию выглядит следуюшим образом:\n",
    "\n",
    "<img align=\"center\" src=\"pictures/doge_main.png\" height=\"600\" width=\"600\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и прежде, для начала сохраним ссылку на страницу в переменную и вытащим по ней контент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meme_page = 'http://knowyourmeme.com/memes/doge'\n",
    "\n",
    "response = requests.get(meme_page, headers={'User-Agent': UserAgent().chrome})\n",
    "\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html,'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как можно вытащить статистику просмотров, комментариев, а также числа загруженных видео о фото, связанных с нашим мемов. Всё это добро хранится справа вверху под тэгами `\"dd\"` и с классами  `\"views\"`, `\"videos\"`, `\"photos\"` и `\"comments\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dd class=\"views\" title=\"12,181,808 Views\">\n",
       "<a href=\"/memes/doge\" rel=\"nofollow\">12,181,808</a>\n",
       "</dd>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "views = soup.find('dd', attrs={'class':'views'})\n",
    "views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12,181,808'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "views = views.find('a').text\n",
    "views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12181808"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "views = int(views.replace(',', ''))\n",
    "views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снова можно запихнуть в небольшую функцию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getStats(soup, stats):\n",
    "    \"\"\"\n",
    "        Возвращает очищенное число просмотров/коментариев/...\n",
    "        \n",
    "        soup: объект bs4.BeautifulSoup \n",
    "            суп текущей страницы\n",
    "            \n",
    "        stats: string\n",
    "            views/videos/photos/comments\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    obj = soup.find('dd', attrs={'class':stats})\n",
    "    obj = obj.find('a').text\n",
    "    obj = int(obj.replace(',', ''))\n",
    "    \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Просмотры: 12181808\n",
      "Видео: 59\n",
      "Фото: 1644\n",
      "Комментарии: 920\n"
     ]
    }
   ],
   "source": [
    "views = getStats(soup, stats='views')\n",
    "videos = getStats(soup, stats='videos')\n",
    "photos = getStats(soup, stats='photos')\n",
    "comments = getStats(soup, stats='comments')\n",
    "\n",
    "print(\"Просмотры: {}\\nВидео: {}\\nФото: {}\\nКомментарии: {}\".format(views, videos, photos, comments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще из инетерсного - достанем дату и время добавления мема. Если посмотреть на страницу в браузере, можно подумать, что максимум информации, который мы можем вытащить - это число лет, прошедших с момента публикации - `Added 4 years ago by NovaXP`. Однако мы так просто сдаваться не \n",
    "будем, полезем в кишки html и откопаем там кусок, ответственный за эту надпись:\n",
    "\n",
    "<img align=\"center\" src=\"pictures/no_shit.jpg\" height=\"300\" width=\"300\"> \n",
    "\n",
    "<img align=\"center\" src=\"pictures/html_time_ago.png\" height=\"600\" width=\"600\"> \n",
    "\n",
    "Ага! Вот и подробности по дате добавления, с точностью до минуты. Элементарно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2017-06-23T15:41:49-04:00'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = soup.find('abbr', attrs={'class':'timeago'}).attrs['title']\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По аналогии напишем функции, вытаскивающие всю остальную информацию со страницы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getProperties(soup):\n",
    "    \"\"\"\n",
    "        Возвращает список (tuple) с названием, статусом, типом, \n",
    "        годом и местом происхождения и тэгами\n",
    "        \n",
    "        soup: объект bs4.BeautifulSoup \n",
    "            суп текущей страницы\n",
    "    \n",
    "    \"\"\"\n",
    "    # название - идёт с самым большим заголовком h1, легко найти\n",
    "    meme_name = soup.find('section', attrs={'class':'info'}).find('h1').text.strip()\n",
    "    \n",
    "    # достаём все данные справа от картинки \n",
    "    properties = soup.find('aside', attrs={'class':'left'})\n",
    "    \n",
    "    # статус идет первым - можно не уточнять класс\n",
    "    meme_status = properties.find(\"dd\")\n",
    "    # oneliner, заменяющий try-except: если тэга нет в properties, вернётся объект NoneType,\n",
    "    # у которого аттрибут text отсутствует, и в этом случае он заменится на пустую строку\n",
    "    meme_status = \"\" if not meme_status else meme_status.text.strip()\n",
    "    \n",
    "    # тип мема - обладает уникальным классом\n",
    "    meme_type = properties.find('a', attrs={'class':'entry-type-link'})\n",
    "    meme_type = \"\" if not meme_type else meme_type.text \n",
    "    \n",
    "    # год происхождения первоисточника можно найти после заголовка Year, \n",
    "    # находим заголовок, определяем родителя и ищем следущего ребенка - наш раздел\n",
    "    meme_origin_year = properties.find(text='\\nYear\\n')\n",
    "    meme_origin_year = \"\" if not meme_origin_year else meme_origin_year.parent.find_next()\n",
    "    meme_origin_year = meme_origin_year.text.strip()\n",
    "    \n",
    "    # сам первоисточник\n",
    "    meme_origin_place = properties.find('dd', attrs={'class':'entry_origin_link'})\n",
    "    meme_origin_place = \"\" if not meme_origin_place else meme_origin_place.text.strip()\n",
    "    \n",
    "    # тэги, связанные с мемом\n",
    "    meme_tags = properties.find('dl', attrs={'id':'entry_tags'}).find('dd')\n",
    "    meme_tags = \"\" if not meme_tags else meme_tags.text.strip()\n",
    "    \n",
    "    return meme_name, meme_status, meme_type, meme_origin_year, meme_origin_place, meme_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Doge',\n",
       " 'Confirmed',\n",
       " 'Character',\n",
       " '2013',\n",
       " 'Tumblr',\n",
       " 'animal, dog, shiba inu, shibe, such doge, super shibe, japanese, super, tumblr, much, very, many, comic sans, photoshop meme, such, shiba, shibe doge, doges, dogges, reddit, comic sans ms, tumblr meme, hacked, bitcoin, dogecoin, shitposting, stare')"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getProperties(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getText(soup):\n",
    "    \"\"\"\n",
    "        Возвращает текстовые описания мема\n",
    "        \n",
    "        soup: объект bs4.BeautifulSoup \n",
    "            суп текущей страницы\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    # достаём все тексты под картинкой\n",
    "    body = soup.find('section', attrs={'class':'bodycopy'})\n",
    "    \n",
    "    # раздел about (если он есть), должен идти первым, берем его без уточнения класса\n",
    "    meme_about = body.find('p')\n",
    "    meme_about = \"\" if not meme_about else meme_about.text\n",
    "    \n",
    "    # раздел origin можно найти после заголовка Origin или History, \n",
    "    # находим заголовок, определяем родителя и ищем следущего ребенка - наш раздел\n",
    "    meme_origin = body.find(text='Origin') or body.find(text='History')\n",
    "    meme_origin = \"\" if not meme_origin else meme_origin.parent.find_next().text\n",
    "    \n",
    "    # весь остальной текст (если он есть) можно запихнуть в одно текстовое поле\n",
    "    if body.text:\n",
    "        other_text = body.text.strip().split('\\n')[4:]\n",
    "        other_text = \" \".join(other_text).strip()\n",
    "    else:\n",
    "        other_text = \"\"\n",
    "        \n",
    "    return meme_about, meme_origin, other_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "О чем мем:\n",
      "Doge is a slang term for “dog” that is primarily associated with pictures of Shiba Inus (nicknamed “Shibe”) and internal monologue captions on Tumblr. These photos may be photoshopped to change the dog’s face or captioned with interior monologues in Comic Sans font.\n",
      "\n",
      "Происхождение:\n",
      "The use of the misspelled word “doge” to refer to a dog dates back to June 24th, 2005, when it was mentioned in an episode of Homestar Runner’s puppet show. In the episode titled “Biz Cas Fri 1”[2], Homestar calls Strong Bad his “d-o-g-e” while trying to distract him from his work.\n",
      "\n",
      "Остальной текст:\n",
      "Identity On February 23rd, 2010, Japanese kindergarten teacher Atsuko Sato posted several photos of her rescue-adopted Shiba Inu dog Kabosu to her personal blog.[38] Among the photos included a peculi...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meme_about, meme_origin, other_text = getText(soup)\n",
    "print(\"О чем мем:\\n{}\\n\\nПроисхождение:\\n{}\\n\\nОстальной текст:\\n{}...\\n\"\\\n",
    "      .format(meme_about, meme_origin, other_text[:200]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, создадим функцию, возвращающую всю информацию по текущему мему"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMemeData(meme_page):\n",
    "    \"\"\"\n",
    "        Запрашивает данные по странице, возвращает обработанный словарь с данными\n",
    "        \n",
    "        meme_page: string\n",
    "            ссылка на страницу с мемом\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # запрашиваем данные по ссылке\n",
    "    response = requests.get(meme_page, headers={'User-Agent': UserAgent().chrome})\n",
    "    \n",
    "    if not response.ok:\n",
    "        # если сервер нам отказал, вернем статус ошибки \n",
    "        return response.status_code\n",
    "    \n",
    "    # получаем содержимое страницы и переводим в суп\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    # используя ранее написанные функции парсим информацию\n",
    "    views = getStats(soup=soup, stats='views')\n",
    "    videos = getStats(soup=soup, stats='videos')\n",
    "    photos = getStats(soup=soup, stats='photos')\n",
    "    comments = getStats(soup=soup, stats='comments')\n",
    "    \n",
    "    # дата\n",
    "    date = soup.find('abbr', attrs={'class':'timeago'}).attrs['title']\n",
    "    \n",
    "    # имя, статус, и т.д.\n",
    "    meme_name, meme_status, meme_type, meme_origin_year, meme_origin_place, meme_tags =\\\n",
    "    getProperties(soup=soup)\n",
    "    \n",
    "    # текстовые поля\n",
    "    meme_about, meme_origin, other_text = getText(soup=soup)\n",
    "    \n",
    "    # составляем словарь, в котором будут хранится все полученные и обработанные данные\n",
    "    data_row = {\"name\":meme_name, \"status\":meme_status, \n",
    "                \"type\":meme_type, \"origin_year\":meme_origin_year, \n",
    "                \"origin_place\":meme_origin_place,\n",
    "                \"date_added\":date, \"views\":views, \n",
    "                \"videos\":videos, \"photos\":photos, \"comments\":comments, \"tags\":meme_tags,\n",
    "                \"about\":meme_about, \"origin\":meme_origin, \"other_text\":other_text}\n",
    "    \n",
    "    return data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_row = getMemeData('http://knowyourmeme.com/memes/doge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь подготовим табличку, чтобы в неё записывать всё ~~награбленные~~ честно полученные данные, добавим в неё первую полученную строку и полюбуемся на результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(columns=['name', 'status', 'type', 'origin_year', 'origin_place',\n",
    "                                 'date_added', 'views', 'videos', 'photos', 'comments', \n",
    "                                 'tags', 'about', 'origin', 'other_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.append(data_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>status</th>\n",
       "      <th>type</th>\n",
       "      <th>origin_year</th>\n",
       "      <th>origin_place</th>\n",
       "      <th>date_added</th>\n",
       "      <th>views</th>\n",
       "      <th>videos</th>\n",
       "      <th>photos</th>\n",
       "      <th>comments</th>\n",
       "      <th>tags</th>\n",
       "      <th>about</th>\n",
       "      <th>origin</th>\n",
       "      <th>other_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Doge</td>\n",
       "      <td>Confirmed</td>\n",
       "      <td>Character</td>\n",
       "      <td>2013</td>\n",
       "      <td>Tumblr</td>\n",
       "      <td>2017-06-23T15:41:49-04:00</td>\n",
       "      <td>12183311</td>\n",
       "      <td>59</td>\n",
       "      <td>1644</td>\n",
       "      <td>920</td>\n",
       "      <td>animal, dog, shiba inu, shibe, such doge, supe...</td>\n",
       "      <td>Doge is a slang term for “dog” that is primari...</td>\n",
       "      <td>The use of the misspelled word “doge” to refer...</td>\n",
       "      <td>Identity On February 23rd, 2010, Japanese kind...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name     status       type origin_year origin_place  \\\n",
       "0  Doge  Confirmed  Character        2013       Tumblr   \n",
       "\n",
       "                  date_added     views videos photos comments  \\\n",
       "0  2017-06-23T15:41:49-04:00  12183311     59   1644      920   \n",
       "\n",
       "                                                tags  \\\n",
       "0  animal, dog, shiba inu, shibe, such doge, supe...   \n",
       "\n",
       "                                               about  \\\n",
       "0  Doge is a slang term for “dog” that is primari...   \n",
       "\n",
       "                                              origin  \\\n",
       "0  The use of the misspelled word “doge” to refer...   \n",
       "\n",
       "                                          other_text  \n",
       "0  Identity On February 23rd, 2010, Japanese kind...  "
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще раз убедимся что всё работает - пройдемся по списку из ссылок мемом, полученных ранее в перменной `meme_links`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for meme_link in meme_links:\n",
    "    data_row = getMemeData(meme_link)\n",
    "    final_df = final_df.append(data_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>status</th>\n",
       "      <th>type</th>\n",
       "      <th>origin_year</th>\n",
       "      <th>origin_place</th>\n",
       "      <th>date_added</th>\n",
       "      <th>views</th>\n",
       "      <th>videos</th>\n",
       "      <th>photos</th>\n",
       "      <th>comments</th>\n",
       "      <th>tags</th>\n",
       "      <th>about</th>\n",
       "      <th>origin</th>\n",
       "      <th>other_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Doge</td>\n",
       "      <td>Confirmed</td>\n",
       "      <td>Character</td>\n",
       "      <td>2013</td>\n",
       "      <td>Tumblr</td>\n",
       "      <td>2017-06-23T15:41:49-04:00</td>\n",
       "      <td>12183311</td>\n",
       "      <td>59</td>\n",
       "      <td>1644</td>\n",
       "      <td>920</td>\n",
       "      <td>animal, dog, shiba inu, shibe, such doge, supe...</td>\n",
       "      <td>Doge is a slang term for “dog” that is primari...</td>\n",
       "      <td>The use of the misspelled word “doge” to refer...</td>\n",
       "      <td>Identity On February 23rd, 2010, Japanese kind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I Killed Them All</td>\n",
       "      <td>Submission</td>\n",
       "      <td></td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2017-11-16T16:22:44-05:00</td>\n",
       "      <td>3540</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>star wars, prequel meme, anakin skywalker, tus...</td>\n",
       "      <td>“I Killed Them All”, also known as “Not Just t...</td>\n",
       "      <td>On May 12th, 2002, Attack of the Clones was re...</td>\n",
       "      <td>“I killed them. I killed them all. They’re dea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Al Franken Sexual Misconduct Allegations</td>\n",
       "      <td>Submission</td>\n",
       "      <td>Controversy</td>\n",
       "      <td>2017</td>\n",
       "      <td>KABC</td>\n",
       "      <td>2017-11-17T17:31:58-05:00</td>\n",
       "      <td>10900</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>30</td>\n",
       "      <td>al franken, leeann tweeden, sexual harassment,...</td>\n",
       "      <td>Al Franken Sexual Misconduct Allegations refer...</td>\n",
       "      <td>On November 16th, 2017, model and sports comme...</td>\n",
       "      <td>Developments After the article was released, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sad Godzilla</td>\n",
       "      <td>Submission</td>\n",
       "      <td></td>\n",
       "      <td>2017</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>2017-11-16T13:59:39-05:00</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>godzilla, monster island buddies</td>\n",
       "      <td>Sad Godzilla is a picture of a sad looking God...</td>\n",
       "      <td></td>\n",
       "      <td>On November 15, 2017, the Youtube channel Mons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thanks For Coming To My TED Talk</td>\n",
       "      <td>Submission</td>\n",
       "      <td>Parody</td>\n",
       "      <td>2015</td>\n",
       "      <td>Tumblr</td>\n",
       "      <td>2017-11-16T13:56:05-05:00</td>\n",
       "      <td>2789</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>tumblr, sherlock, ted talk</td>\n",
       "      <td>Thank You For Coming to My TED Talk is a phras...</td>\n",
       "      <td>Meme Documentation [1] traces the origin of th...</td>\n",
       "      <td>Spread Over the course of the following two ye...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       name      status         type  \\\n",
       "0                                      Doge   Confirmed    Character   \n",
       "1                         I Killed Them All  Submission                \n",
       "2  Al Franken Sexual Misconduct Allegations  Submission  Controversy   \n",
       "3                              Sad Godzilla  Submission                \n",
       "4          Thanks For Coming To My TED Talk  Submission       Parody   \n",
       "\n",
       "  origin_year origin_place                 date_added     views videos photos  \\\n",
       "0        2013       Tumblr  2017-06-23T15:41:49-04:00  12183311     59   1644   \n",
       "1     Unknown      Unknown  2017-11-16T16:22:44-05:00      3540      0     14   \n",
       "2        2017         KABC  2017-11-17T17:31:58-05:00     10900      0     14   \n",
       "3        2017      YouTube  2017-11-16T13:59:39-05:00        99      0      1   \n",
       "4        2015       Tumblr  2017-11-16T13:56:05-05:00      2789      0     11   \n",
       "\n",
       "  comments                                               tags  \\\n",
       "0      920  animal, dog, shiba inu, shibe, such doge, supe...   \n",
       "1        3  star wars, prequel meme, anakin skywalker, tus...   \n",
       "2       30  al franken, leeann tweeden, sexual harassment,...   \n",
       "3        1                   godzilla, monster island buddies   \n",
       "4        4                         tumblr, sherlock, ted talk   \n",
       "\n",
       "                                               about  \\\n",
       "0  Doge is a slang term for “dog” that is primari...   \n",
       "1  “I Killed Them All”, also known as “Not Just t...   \n",
       "2  Al Franken Sexual Misconduct Allegations refer...   \n",
       "3  Sad Godzilla is a picture of a sad looking God...   \n",
       "4  Thank You For Coming to My TED Talk is a phras...   \n",
       "\n",
       "                                              origin  \\\n",
       "0  The use of the misspelled word “doge” to refer...   \n",
       "1  On May 12th, 2002, Attack of the Clones was re...   \n",
       "2  On November 16th, 2017, model and sports comme...   \n",
       "3                                                      \n",
       "4  Meme Documentation [1] traces the origin of th...   \n",
       "\n",
       "                                          other_text  \n",
       "0  Identity On February 23rd, 2010, Japanese kind...  \n",
       "1  “I killed them. I killed them all. They’re dea...  \n",
       "2  Developments After the article was released, F...  \n",
       "3  On November 15, 2017, the Youtube channel Mons...  \n",
       "4  Spread Over the course of the following two ye...  "
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 14)"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично! Всё работает, мемы качаются, данные наполняются и всё было бы хорошо, если бы не одно но - количество запросов, которое нам придётся сделать, чтобы всё получить."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
