{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение или просто добавь воды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commitChanges():\n",
    "    ! git add KnowYourMemesParser.ipynb\n",
    "    ! git commit -m \"added function for full meme page parsing\"\n",
    "    ! git push -u origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commitPictures():\n",
    "    ! git add pictures/\n",
    "    ! git commit -m \"added pictures\"\n",
    "    ! git push -u origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commitChanges()\n",
    "#commitPictures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отсюда начинается реальный парсинг :з\n",
    "\n",
    "Машинное обучение, эконометрика, статистика и многие другие науки о данных занимаются поиском закономерностей. Каждый день доблестные аналитики-инквизиторы пытают природу разными методами и вытаскивают из неё сведенья о том, как именно устроен великий процесс порождения данных, породивший нашу вселенную. Испанские инквизиторы в своей повседневной деятельности пытали непосредственно физическое тело своей жертвы. Природа вездесуща и не имеет однозначного физического облика. Из-за этого профессия современного инквизитора имеет странную специфику. Пытки природы происходят через анализ данных, которые надо откуда-то брать. Обычно данные инквизиторам приносят мирные собиратели. Эта статейка призвана немного приоткрыть завесу тайны с того откуда данные берутся и как их можно немножечко пособирать. \n",
    "\n",
    "Нашим девизом будет знаменитая фраза капитана Джэка Воробья: \"Бери всё и не отдавай ничего\". Собирать для дальнейших исследований и изысканий мы будем мемы. Иногда для их сбора мы будем использовать бандитские методы. Тем не менее, при всём при этом, мы будем оставаться мирными собирателями данных, и ни в коей мере не будем становиться бандитами. Брать мемы мы будем из [мемохранилища.](http://knowyourmeme.com)\n",
    "\n",
    "<img align=\"center\" src=\"https://d.justpo.st/media/images/2014/04/e4eec44d91001b3ab02407d48fe92075.jpg\n",
    "\" height=\"300\" width=\"300\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Вламываемся в мемохранилище\n",
    "\n",
    "## 1.1. Что мы хотим получить\n",
    "\n",
    "Итак, мы хотим распарсить [knowyourmeme.com](http://knowyourmeme.com) и получить кучу разных переменных: \n",
    "\n",
    "- **Name** – название мема,\n",
    "- **Origin_year** – год создания мема,\n",
    "- **Views** – просматриваемость мема,\n",
    "- ** и другие** \n",
    "\n",
    "Более того, мы хотим сделать это без вот этого всего: \n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/FUlyankin/Parsers/master/images%20/2.png\" height=\"600\" width=\"600\">\n",
    "\n",
    "И сделаем. После скачки и чистки данных от мусора, можно будет заняться строительством моделей. Например, попытаться предсказать популярность мема по его параметрам. Но это все позже, а сейчас познакомимся с парой зубодробительных определений:\n",
    "\n",
    "* **Парсер** — это скрипт, который грабит информацию с сайта\n",
    "* **Краулер** — это часть парсера, которая бродит по ссылкам\n",
    "* **Краулинг** — это переход по страницам и ссылкам\n",
    "* **Скрапинг** — это сбор данных со страниц\n",
    "* **Парсинг** — это сразу и краулинг и скрапинг! \n",
    "\n",
    "\n",
    "## 1.2.  Что такое HTML \n",
    "\n",
    "**HTML (HyperText Markup Language)**  — это такой же язык разметки как Markdown или LaTeX. Он является стандартным для написания различных сайтов. Команды в таком языке называются **тегами**. Если открыть абсолютно любой сайт, нажать на правую кнопку мышки, а после нажать `View page source`, то перед вами предстанет HTML скелет этого сайта. \n",
    "\n",
    "Можно увидеть, что HTML-страница это ни что иное как набор вложенных тегов. Можно заметить, например, следующие теги:\n",
    "\n",
    "- `<title>` – заголовок страницы\n",
    "- `<h1>…<h6>` – заголовки разных уровней\n",
    "- `<p>` – абзац (paragraph)\n",
    "- `<div>` – выделения фрагмента документа с целью изменения вида содержимого\n",
    "- `<table>` – прорисовка таблицы \n",
    "- `<tr>` – разделитель для строк в таблице \n",
    "- `<td>` – разделитель для столбцов в таблице\n",
    "- `<b>` – устанавливает жирное начертание шрифта\n",
    "\n",
    "Обычно команда `<...>` открывает тег, а  `</...>` закрывает его. Все, что находится между этими двумя командами, подчиняется правилу, которое диктует тег. Например, все, что находится между `<p>` и  `</p>` — это отдельный абзац.   \n",
    "\n",
    "Теги образуют своеобразное дерево с корнем в теге `<html>` и разбивают страницу на разные логические кусочки. У каждого тега есть свои потомки (дети) — те теги, которые вложены в него и свои родители. \n",
    "\n",
    "Например, HTML-древо страницы может выглядеть вот так:\n",
    "\n",
    "    <html>\n",
    "    <head> Заголовок </head>\n",
    "    <body>\n",
    "        <div> \n",
    "            Первый кусок текста со своими свойствами\n",
    "        </div>\n",
    "        <div>\n",
    "            Второй кусок текста\n",
    "                <b>\n",
    "                    Третий, жирный кусок\n",
    "                </b>\n",
    "        </div>\n",
    "        Четвёртый кусок текста        \n",
    "    </body>\n",
    "    </html>            \n",
    "    \n",
    "    \n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/FUlyankin/Parsers/master/images%20/tree.png\" height=\"500\" width=\"500\"> \n",
    "\n",
    "Можно работать с этим html как с тексом, а можно как с деревом. Обход этого дерева и есть парсинг веб-страницы. Мы всего лишь будем находить нужные нам узлы среди всего этого разнообразия и забирать из них информацию!\n",
    "\n",
    "Вручную обходить эти деревья не очень приятно, поэтому есть специальные языки для обхода деревьев.\n",
    "\n",
    "- [CSS-селектор](https://ru.wikibooks.org/wiki/CSS/%D0%A1%D0%B5%D0%BB%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D1%8B) (это когда мы ищем элемент страницы по паре ключ, значение)\n",
    "- [XPath](https://ru.wikipedia.org/wiki/XPath) (это когда мы прописываем путь по дереву вот так: /html/body/div[1]/div[3]/div/div[2]/div)\n",
    "- Всякие разные библиотеки для всяких разных языков, например, BeautifulSoup для питона. Именно эту библиотеку мы и будем использовать. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Наш первый запрос\n",
    "\n",
    "Доступ к веб-станицам позволяет получать модуль `requests`. Подгрузим его. За компанию подгрузим ещё парочку дельных пакетов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests      # Библиотека для отправки запросов\n",
    "import numpy as np   # Библиотека для матриц, векторов и линала\n",
    "import pandas as pd  # Библиотека для табличек \n",
    "import time          # Библиотека для времени"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также подгрузим небольшой сюрприз. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a surprise tool that will help us later!\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для наших благородных исследовательских целей нужно собрать данные по каждому мему с соответствующей ему страницы. Но для начала нужно получить адреса этих страниц. Поэтому открываем основную страницу со всеми выложенными мемами. Выглядит она следующим образом:\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/DmitrySerg/memology/master/pictures/memes_main.png\" height=\"500\" width=\"500\"> \n",
    "\n",
    "Отсюда мы и будем тащить ссылки на каждый из перечисленных мемов. Сохраним в переменную `page_link` адрес основной страницы и откроем её при помощи библиотеки `requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_link = 'http://knowyourmeme.com/memes/all/page/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(page_link)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот и первая проблема! Обращаемся к [главному источнику знаний](https://en.wikipedia.org/wiki/HTTP_403) и выясняем, что 403-я ошибка выдается сервером, если он доступен и способен обрабатывать запросы, но по некоторым личным причинам отказывается это делать. \n",
    "\n",
    "**Картинка с котиком 403** \n",
    "\n",
    "Попробуем выяснить, почему. Для этого посмотрим, как выглядел финальный запрос, отправленный нами на сервер, а в первую очередь - как выглядел наш User-Agent в глазах сервера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in response.request.headers.items():\n",
    "    print(key+\": \"+value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Диме: надо отмониторите номер библиотеки!**\n",
    "\n",
    "Похоже, мы недвусмысленно дали понять серверу, что мы сидим на питоне и используем библиотеку requests под версией 2.14.2. Скорее всего, это вызвало у сервера некоторые подозрения относительно наших благих намерений и он решил нас безжалостно отвергнуть. Для сравнения, можно посмотреть, как выглядят request-headers у здорового человека:\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/DmitrySerg/memology/master/pictures/good_headers.png\" height=\"800\" width=\"800\"> \n",
    "\n",
    "Очевидно, что нашему скромному запросу не тягаться с таким обилием мета-информации, которое передается при запросе из обычного браузера. К счастью, никто нам не мешает притвориться человечными и пустить пыль в глаза сервера при помощи генерации фейкового юзер-агента. Библиотек, которые справляются с такой задачей, существует очень и очень много, лично мне больше всего нравится [`fake-useragent`](https://pypi.python.org/pypi/fake-useragent). При вызове метода из различных кусочков будет генерироваться рандомное сочетание операционной системы, спецификаций и версии браузера, которые можно передавать в запрос:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UserAgent().chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(page_link, headers={'User-Agent': UserAgent().chrome})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Замечательно, наша небольшая маскировка сработала и обманутый сервер покорно выдал благословенный 200 ответ — соединение установлено и данные получены, всё чудесно! Посмотрим, что же все-таки мы получили."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит неудобоваримо, как насчет сварить из этого дела что-то покрасивее? Например, красивый суп."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Красивый суп\n",
    "\n",
    "<img align=\"center\" src=\"https://www.crummy.com/software/BeautifulSoup/10.1.jpg\" height=\"200\" width=\"200\"> \n",
    "\n",
    "Пакет **[bs4 , a.k.a BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/)** (тут есть гиперссылка на лучшего друга человека — документацию) был назван в честь стишка про красивый суп из Алисы в стране чудес.\n",
    "\n",
    "Красивый суп — это совершенно волшебная библиотека, которая из сырого и необработанного HTML кода страницы выдаст вам структурированный массив данных, по которому очень удобно искать необходимые теги, классы, атрибуты, тексты и прочие элементы веб страниц.\n",
    "\n",
    "> Пакет под названием `BeautifulSoup` — скорее всего, не то, что нам нужно. Это третья версия (*Beautiful Soup 3*), а мы будем использовать четвертую. Нужно будет установить пакет `beautifulsoup4`. Чтобы было совсем весело, при импорте нужно указывать другое название пакета — `bs4`, а импортировать функцию под названием `BeautifulSoup`. В общем, сначала легко запутаться, но эти трудности нужно преодолеть.\n",
    "\n",
    "С необработанным XML кодом страницы пакет также работает (XML — это искаверканый и превращённый в диалект, с помощью своих команд, HTML). Для того, чтобы пакет корректно работал с XML разметкой, придётся в довесок ко всему нашему арсеналу установить пакет `xml`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Передадим функции `BeautifulSoup` текст веб-страницы, которую мы недавно получили."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html,'html.parser') # В опции также можно указать lxml, \n",
    "                                         # если предварительно установить одноименный пакет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим что-то вот такое:\n",
    "    \n",
    "```\n",
    "<!DOCTYPE html>\n",
    "\n",
    "<html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:fb=\"http://www.facebook.com/2008/fbml\">\n",
    "<head>\n",
    "<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
    "<script type=\"text/javascript\">window.NREUM||(NREUM={});NREUM.info={\"beacon\":\"bam.nr-data.net\",\"errorBeacon\":\"bam.nr-data.net\",\"licenseKey\":\"c1a6d52f38\",\"applicationID\":\"31165848\",\"transactionName\":\"dFdfRUpeWglTQB8GDUNKWFRLHkUNWUU=\",\"queueTime\":0,\"applicationTime\":24,\"agent\":\"\"}</script>\n",
    "<script type=\"text/javascript\">window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var o=n[t]={exports:{}};e[t][0].call(o.exports,function(n){var o=e[t][1][n];return r(o||n)},o,o.exports)}return n[t].exports}if(\"function\"==typeof __nr_require)return __nr_require;for(var o=0;o<t.length;o++)r(t[o]);return r}({1:[function(e,n,t){function r(){}function o(e,n,t){return function(){return i(e,[c.now()].concat(u(arguments)),n?null:this,t),n?void 0:this}}var i=e(\"handle\"),a=e(2),u=e(3),f=e(\"ee\").get(\"tracer\"),c=e(\"loader\"),s=NREUM;\"undefined\"==typeof window.newrelic&&(newrelic=s);var p=\n",
    "```\n",
    "\n",
    "Стало намного лучше, не правда ли? Что же лежит в переменной `soup`? Невнимательный пользователь, скорее всего, скажет,что ничего вообще не изменилось. Тем не менее, это не так. Теперь мы можем свободно бродить по HTML-дереву страницы, искать детей, родителей и вытаскивать их! \n",
    "\n",
    "Например, можно бродить по вершинам, указывая путь из тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.html.head.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно вытащить из того места, куда мы забрели, текст с помощью метода `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.html.head.title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более того, зная адрес элемента, мы сразу можем найти его. Например, можно сделать это по классу. Следующая команда должна найти элемент, который лежит внутри тега `a` и имеет класс `photo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = soup.find('a', attrs = {'class':'photo'})\n",
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, вопреки нашим ожиданиям, вытащенный объект имеет класс `\"photo left\"`. Оказывается, `BeautifulSoup4` расценивает аттрибуты `class` как набор отдельных значений, поэтому `\"photo left\"` для библиотеки равносильно `[\"photo\", \"left\"]`, а указанное нами значение этого класса `\"photo\"` входит в этот список. Чтобы избежать такой неприятной ситуации и проходов по ненужным нам ссылкам, придется воспользоваться собственной функцией и задать строгое соответствие:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = soup.find(lambda tag: tag.name == 'a' and tag.get('class') == ['photo'])\n",
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученный после поиска объект также обладает структурой bs4. Поэтому можно продолжить искать нужные нам объекты уже в нём! Вытащим ссылку на этот мем. Сделать это можно по атрибуту `href`, в котором лежит наша ссылка. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.attrs['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что после всех этих безумных преобразований у данных поменялся тип. Теперь они `str`. Это означет, что с ними можно работать как с текстом и пускать в ход для отсеивания лишней информации регулярные выражения. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Тип данных до вытаскивания ссылки:\", type(obj))\n",
    "print(\"Тип данных после вытаскивания ссылки:\", type(obj.attrs['href']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если несколько элементов на странице обладают указанным адресом, то метод `find` вернёт только самый первый.  Чтобы найти все элементы с таким адресом, нужно использовать метод `findAll`. На выход будет выдан список! Таким образом, мы можем получить одним поиском сразу все объекты, содержащие ссылки на страницы с мемами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_links = soup.findAll(lambda tag: tag.name == 'a' and tag.get('class') == ['photo'])\n",
    "meme_links[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось очистить полученный список от мусора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_links = [link.attrs['href'] for link in meme_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_links[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(meme_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готово, получили ровно 16 ссылок по числу мемов на одной странице поиска. \n",
    "\n",
    "Окей, то, что можно искать элемент по его адресу, конечно же, круто, но откуда взять этот адрес? Можно установить для своего браузера какую-нибудь утилиту, позволяющую вытаскивать со страницы нужные теги. Например, [selectorgadget.](http://selectorgadget.com/)\n",
    "\n",
    "Тем не менее, этот путь не подходит для истинного самурая. Для последователей бусидо есть другой способ — искать теги для каждого нужного нам элемента вручную. Для этого нам придётся жать правой кнопкой мышки по окошечку браузера и тыкать кнопку **Inspect**. Ваше окно браузера будет после всех этих манипуляций выглядеть как-то вот так: \n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/DmitrySerg/memology/master/pictures/memes_inspection.png\" height=\"800\" width=\"800\"> \n",
    "\n",
    "Если тыкнуть кнопочку, которая находится в огромном красном кружочке, а после навести курсорчик на требующийся вам кусочек странички и тыкнуть на него, то справа, там, где находится гигантская стрелочка, выскочит кусок html-кода, в котором находится адрес выбранного вами объекта. Этот адрес можно смело копировать в код и наслаждаться своей брутальностью. \n",
    "\n",
    "Остался последний момент. Когда мы скачаем все мемы с текущей страницы, нам нужно будет каким-то образом забраться на соседнюю. На сайте это можно делать просто пролистывая страничку с мемами вниз.\n",
    "\n",
    "Обычно, все параметры, которые мы устанавливаем на сайте для поиска, отображаются на структуре хрефа. Мемы не являются исключением. Если мы хотим получить первую порцию из мемов, мы должны будем обратиться к сайту по ссылке \n",
    "\n",
    "                `http://knowyourmeme.com/memes/all/page/1`\n",
    "\n",
    "\n",
    "Если мы захотим получить поцию из вторых 16 мемов, нам придётся немного видоизменить ссылку, а именно заменить номер страницы на второй.\n",
    "\n",
    "\n",
    "                `http://knowyourmeme.com/memes/all/page/2`\n",
    " \n",
    "Таким незатейливым образом мы сможем пройтись по всем страницам и ограбить мемохранилище. \n",
    "\n",
    "Наконец, обернем в красивую функцию все-все манипуляции, проделанные выше:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getPageLinks(page_number):\n",
    "    \"\"\"\n",
    "        Возвращает список ссылок на мемы, полученный с текущей страницы\n",
    "        \n",
    "        page_number: int/string\n",
    "            номер страницы для парсинга\n",
    "            \n",
    "    \"\"\"\n",
    "    # составляем ссылку на страницу поиска\n",
    "    page_link = 'http://knowyourmeme.com/memes/all/page/{}'.format(page_number)\n",
    "    \n",
    "    # запрашиваем данные по ней\n",
    "    response = requests.get(page_link, headers={'User-Agent': UserAgent().chrome})\n",
    "    \n",
    "    if not response.ok:\n",
    "        # если сервер нам отказал, вернем пустой лист для текущей страницы\n",
    "        return [] \n",
    "    \n",
    "    # получаем содержимое страницы и переводим в суп\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    # наконец, ищем ссылки на мемы и очищаем их от ненужных тэгов\n",
    "    meme_links = soup.findAll(lambda tag: tag.name == 'a' and tag.get('class') == ['photo'])\n",
    "    meme_links = ['http://knowyourmeme.com' + link.attrs['href'] for link in meme_links]\n",
    "    \n",
    "    return meme_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем функцию и убедимся, что всё хорошо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_links = getPageLinks(1)\n",
    "meme_links[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_links = getPageLinks(2)\n",
    "meme_links[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, функция работает и теперь мы теоретически можем достать ссылки на все $17171$ мем, для чего нам придется пройтись по $\\frac{17171}{16} \\approx 1074$ страницам. Прежде чем расстраивать сервер таким количеством запросов, посмотрим, как доставать всю необходимую информацию о конкретном меме. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Финальная подготовка к грабежу\n",
    "\n",
    "По аналогии со ссылками можно вытащить что угодно. Для этого надо сделать несколько шагов: \n",
    "\n",
    "1. Открываем страничку с мемом\n",
    "2. Находим тег для нужной нам информации\n",
    "3. Пихаем всё это в красивый суп\n",
    "4. ......\n",
    "5. Profit \n",
    "\n",
    "Для закрепления информации в голове любознательного читателя, вытащим число просмотров мема.\n",
    "\n",
    "А в качестве примера возьмем самый популярный на этом сайте мем - Doge, набравший более 12 миллионов просмотров по состоянию на 18 ноября 2017 года. \n",
    "\n",
    "Сама страница, с которой мы будем доставать дорогую нашему исследовательскому сердцу информацию выглядит следуюшим образом:\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/DmitrySerg/memology/master/pictures/doge_main.png\" height=\"600\" width=\"600\"> \n",
    "\n",
    "Как и прежде, для начала сохраним ссылку на страницу в переменную и вытащим по ней контент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_page = 'http://knowyourmeme.com/memes/doge'\n",
    "\n",
    "response = requests.get(meme_page, headers={'User-Agent': UserAgent().chrome})\n",
    "\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html,'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как можно вытащить статистику просмотров, комментариев, а также числа загруженных видео о фото, связанных с нашим мемов. Всё это добро хранится справа вверху под тэгами `\"dd\"` и с классами  `\"views\"`, `\"videos\"`, `\"photos\"` и `\"comments\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = soup.find('dd', attrs={'class':'views'})\n",
    "views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = views.find('a').text\n",
    "views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = int(views.replace(',', ''))\n",
    "views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снова апихнём всё это в небольшую функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getStats(soup, stats):\n",
    "    \"\"\"\n",
    "        Возвращает очищенное число просмотров/коментариев/...\n",
    "        \n",
    "        soup: объект bs4.BeautifulSoup \n",
    "            суп текущей страницы\n",
    "            \n",
    "        stats: string\n",
    "            views/videos/photos/comments\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    obj = soup.find('dd', attrs={'class':stats})\n",
    "    obj = obj.find('a').text\n",
    "    obj = int(obj.replace(',', ''))\n",
    "    \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всё готово! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = getStats(soup, stats='views')\n",
    "videos = getStats(soup, stats='videos')\n",
    "photos = getStats(soup, stats='photos')\n",
    "comments = getStats(soup, stats='comments')\n",
    "\n",
    "print(\"Просмотры: {}\\nВидео: {}\\nФото: {}\\nКомментарии: {}\".format(views, videos, photos, comments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще из инетерсного —  достанем дату и время добавления мема. Если посмотреть на страницу в браузере, можно подумать, что максимум информации, который мы можем вытащить - это число лет, прошедших с момента публикации —  `Added 4 years ago by NovaXP`. Однако мы так просто сдаваться не будем, полезем в кишки html и откопаем там кусок, ответственный за эту надпись:\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/DmitrySerg/memology/master/pictures/no_shit.jpg\" height=\"300\" width=\"300\"> \n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/DmitrySerg/memology/master/pictures/html_time_ago.png\" height=\"600\" width=\"600\"> \n",
    "\n",
    "Ага! Вот и подробности по дате добавления, с точностью до минуты. Элементарно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = soup.find('abbr', attrs={'class':'timeago'}).attrs['title']\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле, парсеры — дело непредсказуемое. Часто страницы, которые мы парсим, имеют очень неоднородну структуру. Например, если мы парсим мемы, на части страниц может быть указано описание, а на части нет. Как только код впервые натыкается на отсутствие описания, он выдаёт ошибку и останавливается. Чтобы нормально собрать все данные, приходится прописывать исключения. Вроде бы, хранилище мемов хорошо оборудовано и никаких внештатных ситуаций происходить не должно. Тем не менее, очень не хочется проснуться утром и увидеть, что код сделал 20 итераций, нарвался на ошибку и отрубился.  Чтобы такого не произошло, мы будем использовать на каждой итерации конструкцию `try - except`, которая часто выручает при собирательстве. \n",
    "\n",
    "Если кто не знает, `try - except` — это очень крутая конструкция, которая позволяет прописывать исключения. Посмотрим на то как она работает: \n",
    "\n",
    "```python\n",
    "try:\n",
    "    something\n",
    "except Name_of_some_error:\n",
    "    do_something_else\n",
    "```\n",
    "\n",
    "> Конструкция позволяет в случае, если произошла ошибка типа Name_of_some_error не заканчивать программу со словами «Все пропало! Ошибка!», а тут же передать управление блоку do_something_else, который что-нибудь сделает. \n",
    "\n",
    "Например, в ситуации ниже, когда функция сталкивается с отрицательными числами, она не пишет нам, что логарифма от отрицательного числа не существует, она молча записывает в вектор `b` указанное нами число. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "a = [1,2,3,4,-5,-7,9,10,0,12,2]\n",
    "b = [ ]\n",
    "for atem in a:\n",
    "    try:\n",
    "        b.append(log(atem))\n",
    "    except:\n",
    "        b.append('NA')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Про исключения можно почитать [на просорах интернета](https://pythonworld.ru/tipy-dannyx-v-python/isklyucheniya-v-python-konstrukciya-try-except-dlya-obrabotki-isklyuchenij.html). В данном примере мы наложили очень сильное исключение, которое работает при возникновении абсолютно любой неадекватной ситуации в коде. Это исключение при необходимость можно ослабить, заменив на другое. \n",
    "\n",
    "Например, код ниже пытается найти статус мема и успешно находит. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = soup.find('aside', attrs={'class':'left'})\n",
    "meme_status = properties.find(\"dd\")\n",
    "meme_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше нужно вытащить из тэгов текст, а после обрубить все лишние пробелы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_status.text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, если неожиданно выяснится, что у мема нет статуса, метод `find` вернёт пустоту. Метод `text`, в свою очередь, не сможет найти в тэгах текст и выдаст ошибку. Чтобы обезопасить себя от таких пустот, можно прописать исключение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Делай раз! Находим статус мема!\n",
    "properties = soup.find('aside', attrs={'class':'left'})\n",
    "meme_status = properties.find(\"dd\")\n",
    "\n",
    "# Делай два! Пытаемся вытащить его \n",
    "try:\n",
    "    print(meme_status.text.strip()) \n",
    "# Ежели возникает ошибка, статус не найден, выдаём пустоту.\n",
    "except:\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой код позволяет обезопасить себя от ошибок в коде. В данном случае, мы можем переписать всю конструкцию с исключениями в виде одной строки. Эта строка проверит полон ли респонса `meme_status` и ежели нет, то выдаст пустоту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_status = \"\" if not meme_status else meme_status.text.strip()\n",
    "print(meme_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По аналогии можно вытащить всю остальную информацию со страницы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getProperties(soup):\n",
    "    \"\"\"\n",
    "        Возвращает список (tuple) с названием, статусом, типом, \n",
    "        годом и местом происхождения и тэгами\n",
    "        \n",
    "        soup: объект bs4.BeautifulSoup \n",
    "            суп текущей страницы\n",
    "    \n",
    "    \"\"\"\n",
    "    # название - идёт с самым большим заголовком h1, легко найти\n",
    "    meme_name = soup.find('section', attrs={'class':'info'}).find('h1').text.strip()\n",
    "    \n",
    "    # достаём все данные справа от картинки \n",
    "    properties = soup.find('aside', attrs={'class':'left'})\n",
    "    \n",
    "    # статус идет первым - можно не уточнять класс\n",
    "    meme_status = properties.find(\"dd\")\n",
    "    # oneliner, заменяющий try-except: если тэга нет в properties, вернётся объект NoneType,\n",
    "    # у которого аттрибут text отсутствует, и в этом случае он заменится на пустую строку\n",
    "    meme_status = \"\" if not meme_status else meme_status.text.strip()\n",
    "    \n",
    "    # тип мема - обладает уникальным классом\n",
    "    meme_type = properties.find('a', attrs={'class':'entry-type-link'})\n",
    "    meme_type = \"\" if not meme_type else meme_type.text \n",
    "    \n",
    "    # год происхождения первоисточника можно найти после заголовка Year, \n",
    "    # находим заголовок, определяем родителя и ищем следущего ребенка - наш раздел\n",
    "    meme_origin_year = properties.find(text='\\nYear\\n')\n",
    "    meme_origin_year = \"\" if not meme_origin_year else meme_origin_year.parent.find_next()\n",
    "    meme_origin_year = meme_origin_year.text.strip()\n",
    "    \n",
    "    # сам первоисточник\n",
    "    meme_origin_place = properties.find('dd', attrs={'class':'entry_origin_link'})\n",
    "    meme_origin_place = \"\" if not meme_origin_place else meme_origin_place.text.strip()\n",
    "    \n",
    "    # тэги, связанные с мемом\n",
    "    meme_tags = properties.find('dl', attrs={'id':'entry_tags'}).find('dd')\n",
    "    meme_tags = \"\" if not meme_tags else meme_tags.text.strip()\n",
    "    \n",
    "    return meme_name, meme_status, meme_type, meme_origin_year, meme_origin_place, meme_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getProperties(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Свойства мема собрали. Теперь собираем по аналогии его текстовое описание. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getText(soup):\n",
    "    \"\"\"\n",
    "        Возвращает текстовые описания мема\n",
    "        \n",
    "        soup: объект bs4.BeautifulSoup \n",
    "            суп текущей страницы\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    # достаём все тексты под картинкой\n",
    "    body = soup.find('section', attrs={'class':'bodycopy'})\n",
    "    \n",
    "    # раздел about (если он есть), должен идти первым, берем его без уточнения класса\n",
    "    meme_about = body.find('p')\n",
    "    meme_about = \"\" if not meme_about else meme_about.text\n",
    "    \n",
    "    # раздел origin можно найти после заголовка Origin или History, \n",
    "    # находим заголовок, определяем родителя и ищем следущего ребенка - наш раздел\n",
    "    meme_origin = body.find(text='Origin') or body.find(text='History')\n",
    "    meme_origin = \"\" if not meme_origin else meme_origin.parent.find_next().text\n",
    "    \n",
    "    # весь остальной текст (если он есть) можно запихнуть в одно текстовое поле\n",
    "    if body.text:\n",
    "        other_text = body.text.strip().split('\\n')[4:]\n",
    "        other_text = \" \".join(other_text).strip()\n",
    "    else:\n",
    "        other_text = \"\"\n",
    "        \n",
    "    return meme_about, meme_origin, other_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_about, meme_origin, other_text = getText(soup)\n",
    "\n",
    "print(\"О чем мем:\\n{}\\n\\nПроисхождение:\\n{}\\n\\nОстальной текст:\\n{}...\\n\"\\\n",
    "      .format(meme_about, meme_origin, other_text[:200]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, создадим функцию, возвращающую всю информацию по текущему мему"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getMemeData(meme_page):\n",
    "    \"\"\"\n",
    "        Запрашивает данные по странице, возвращает обработанный словарь с данными\n",
    "        \n",
    "        meme_page: string\n",
    "            ссылка на страницу с мемом\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # запрашиваем данные по ссылке\n",
    "    response = requests.get(meme_page, headers={'User-Agent': UserAgent().chrome})\n",
    "    \n",
    "    if not response.ok:\n",
    "        # если сервер нам отказал, вернем статус ошибки \n",
    "        return response.status_code\n",
    "    \n",
    "    # получаем содержимое страницы и переводим в суп\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "    # используя ранее написанные функции парсим информацию\n",
    "    views = getStats(soup=soup, stats='views')\n",
    "    videos = getStats(soup=soup, stats='videos')\n",
    "    photos = getStats(soup=soup, stats='photos')\n",
    "    comments = getStats(soup=soup, stats='comments')\n",
    "\n",
    "    # дата\n",
    "    date = soup.find('abbr', attrs={'class':'timeago'}).attrs['title']\n",
    "\n",
    "    # имя, статус, и т.д.\n",
    "    meme_name, meme_status, meme_type, meme_origin_year, meme_origin_place, meme_tags =\\\n",
    "    getProperties(soup=soup)\n",
    "\n",
    "    # текстовые поля\n",
    "    meme_about, meme_origin, other_text = getText(soup=soup)\n",
    "\n",
    "    # составляем словарь, в котором будут хранится все полученные и обработанные данные\n",
    "    data_row = {\"name\":meme_name, \"status\":meme_status, \n",
    "                \"type\":meme_type, \"origin_year\":meme_origin_year, \n",
    "                \"origin_place\":meme_origin_place,\n",
    "                \"date_added\":date, \"views\":views, \n",
    "                \"videos\":videos, \"photos\":photos, \"comments\":comments, \"tags\":meme_tags,\n",
    "                \"about\":meme_about, \"origin\":meme_origin, \"other_text\":other_text}\n",
    "\n",
    "    return data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_row = getMemeData('http://knowyourmeme.com/memes/doge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь подготовим табличку, чтобы в неё записывать всё ~~награбленные~~ честно полученные данные, добавим в неё первую полученную строку и полюбуемся на результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(columns=['name', 'status', 'type', 'origin_year', 'origin_place',\n",
    "                                 'date_added', 'views', 'videos', 'photos', 'comments', \n",
    "                                 'tags', 'about', 'origin', 'other_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.append(data_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый мем оказался в наших рукак. Именно в этот момент мы понимаем, что покой это ложь — есть только страсть. В текущий момент — это страсть к сбору мемов. Еще раз убедимся что всё работает — пройдемся по списку из ссылок мемом, полученных ранее в перменной `meme_links`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for meme_link in meme_links:\n",
    "    data_row = getMemeData(meme_link)\n",
    "    final_df = final_df.append(data_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично! Всё работает, мемы качаются, данные наполняются и всё было бы хорошо, если бы не одно но — количество запросов, которое нам придётся сделать, чтобы всё получить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Когда работающий код больше не работает\n",
    "\n",
    "Вот он! Тот самый момент абсолютного триумфа, когда код дописан и всё, что нам, мирным собирателям, остаётся — запустить наш код на одну ночку. Кажется, что через страсть мы преобрели силу. Запускаем наш код по всем $1075$ страницам с мемами. На всякий случай обернём наш цикл в `try-except`. Мало ли что там с этими мемами бывает. \n",
    "\n",
    "\n",
    "**Для Димы: какого-то хера остаются проблемы с некоторыми мемами. Я так и не смог их пофиксить (((**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Немного пафосных циклов. При желании пакет можно отключить и \n",
    "# удалить команду tqdm_notebook из всех циклов\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "final_df = pd.DataFrame(columns=['name', 'status', 'type', 'origin_year', 'origin_place',\n",
    "                                 'date_added', 'views', 'videos', 'photos', 'comments', \n",
    "                                 'tags', 'about', 'origin', 'other_text'])\n",
    "\n",
    "\n",
    "for page_number in tqdm_notebook(range(1075)):\n",
    "    meme_links = getPageLinks(page_number)               # собрали хрефы с текущей страницы\n",
    "    for meme_link in meme_links:\n",
    "        try:\n",
    "            data_row = getMemeData(meme_link)            # пытаемся собрать по мему немного даты\n",
    "            final_df = final_df.append(data_row, ignore_index=True)  # и закидываем её в таблицу\n",
    "        except:\n",
    "            # Иначе, ахтунг! \n",
    "            print('AHTUNG! Meme not found:', meme_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сон был прекрасным! Солце только-только взошло из-за горизонта, мы уже бежим за компьютер смотреть мемы и видим, что огромное число мемов не скачалось.\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/DmitrySerg/memology/master/pictures/work_or_not.jpg\" height=\"500\" width=\"500\">\n",
    "\n",
    "Конечно же, вполне естественной реакцией будет нажать на первую же ссылку, перейти в мемохранилище и увидеть, что нас забанили.\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/DmitrySerg/memology/master/pictures/memes_ban.png\" height=\"600\" width=\"600\"> \n",
    "\n",
    "Все наши реквесты остались без респонсов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMemeData('http://knowyourmeme.com/memes/doge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Тор — сын Одина\n",
    "\n",
    "Вопреки пропагандируемому мнению, Tor используется не только преступниками, педофилами и прочими нехорошими террористами. Это, мягко говоря, далеко не так, и мы, мирные собиратели данных, являемся тому подтверждением. Иногда серверу надоедает общаться с одним и тем же человеком, делающим кучу запросов и сервер банит его. К сожалению, не только люди обладают вздорным характером... \n",
    "\n",
    "Приходится маскироваться. Для такой маскировки довольно удобно использовать возможности Tor. Всем прелестям, связанным с работой Tor, можно было бы посвятить несколько больших статеек, что собственно говоря уже и сделано. Подробнее про это можно почитатать по следующим ссылкам: \n",
    "\n",
    "* ссылки \n",
    "\n",
    "Мы же ограничимся только функциональной частью, а именно без углубления в детали опишем шаги, которые нужно предпринять для того, чтобы использовать возможности Tor для обхода блокировки. Для начала полюбуемся на свой ip-адрес. Для этого сделаем get-запрос к сайту, который был создан злобными школьниками (если кто не понял, он знает наш ip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get('http://icanhazip.com').content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменить свой ip через Tor можно двумя путями. Сложным и более простым, если со сложным у нас не очень сложилось. Сначала попробуем пойти более простой, браузерной дорогой.\n",
    "\n",
    "Скачаем браузерный [tor,](https://www.torproject.org/download/download) чтобы лёгкий путь был совсем прост. Для сложного пути в довесок к браузеру поставим tor через консоль. На линуксе нам поможет команда `apt-get install tor`, на маке сделаем это [в рамках brew](https://www.torproject.org/docs/tor-doc-osx.html.en), `brew install tor`. На windows нам поможет переустановка операционной системы. \n",
    "\n",
    "## 3.1 Путь первый \n",
    "\n",
    "Теперь открываем браузер и оставляем его открытым.  Менять ip нам поможет библиотека `PySocks`. Конечно же её нужно установить, скопировав в терминал `pip3 install PySocks`.  \n",
    "\n",
    "**Диме:  бла бла бла как это работает (я хз как это описать без слова \"магия\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socks\n",
    "import socket\n",
    "socks.set_default_proxy(socks.SOCKS5, \"localhost\", 9150)\n",
    "socket.socket = socks.socksocket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на свой новый ip-aдрес. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get('http://icanhazip.com', headers={'User-Agent': UserAgent().chrome}).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При сопутствующем желании, [можно выяснить](http://ru.smart-ip.net/geoip) из какой страны в данный момент мы сидим в интернете.\n",
    "\n",
    "Ого... Германия.\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/FUlyankin/Parsers/master/images%20/ip_adress.png\" height=\"250\" width=\"250\"> \n",
    "\n",
    "Попробуем обратиться к мемохранилищу с нового ip-адреса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "getMemeData('http://knowyourmeme.com/memes/doge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бан снят. Стражники мемов ничего не заподозрили и пустили нас в сокровищницу. Чашу нашего респонса снова переполняет контент. Через силу мы обрели мощь. \n",
    "\n",
    "При желании, можно выяснить одну занимательную вещь: при базовых настройках, Тор-браузер меняет ip каждые 10 минут. Но что делать, если сервер банит нас быстрее?  Всё просто, в папке, куда был установлен Tor найдём файлик с настройками под названием torrc и отредактируем его. Добавим строки: \n",
    "\n",
    "```\n",
    "CircuitBuildTimeout 10\n",
    "LearnCircuitBuildTimeout 0\n",
    "MaxCircuitDirtiness 10\n",
    "```\n",
    "\n",
    "Минимально возможные период для обновления ip составляет 10 секунд. Установим туда эту цифру и попробуем поиграться. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(requests.get('http://icanhazip.com').content)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действительно, смена ip происходит раз в 10 секунд. Для наших целей по скачке мемов было достаточно и базовых настроек. Бан наступал примерно через 20 минут после начала работы кода. \n",
    "\n",
    "1. Открываем браузер;\n",
    "2. Запускаем кусок кода с подгрузкой библиотек;\n",
    "3. Запускаем цикл по мемам \n",
    "4. .....\n",
    "5. Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(columns=['name', 'status', 'type', 'origin_year', 'origin_place',\n",
    "                                 'date_added', 'views', 'videos', 'photos', 'comments', \n",
    "                                 'tags', 'about', 'origin', 'other_text'])\n",
    "\n",
    "\n",
    "for page_number in tqdm_notebook(range(1075)):\n",
    "    meme_links = getPageLinks(page_number)               # собрали хрефы с текущей страницы\n",
    "    for meme_link in meme_links:\n",
    "        try:\n",
    "            data_row = getMemeData(meme_link)            # пытаемся собрать по мему немного даты\n",
    "            final_df = final_df.append(data_row, ignore_index=True)  # и закидываем её в таблицу\n",
    "        except:\n",
    "            # Иначе, ахтунг! \n",
    "            print('AHTUNG! Meme not found:', meme_link)\n",
    "            \n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все мемы в наших руках. Можно приступать к варки фичей и моделированию. Через мощь мы познали победу. Остался только один вопрос: **Что, если мы хотим менять ip каждый реквест?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Пуь второй \n",
    "\n",
    "Второй путь помогает извращаться со сменой ip как угодно. Зайдём на [Github каких-то ребят](https://github.com/alex-miller-0/Tor_Crawler) и скачаем себе их скрипт под названием `TorCrawler.py`. Все недостающие библиотеки, используемые в этом скрипте, придётся доставить. Поставим парням на репозиторий за их код звёздочку. Закинем этот скрипт либо в папку со своими библиотеками либо в папку к этому блокноту. Обратите внимание, что скрипт работает только с третьим питоном. \n",
    "\n",
    "Перед использованием библиотечки, нужно подкрутить настройки в torrc файлике. На маке он будет лежать в папке `/usr/local/etc/tor/`, на линуксе в папке `/etc/tor/`. Проследуем по инструкции авторов скрипта. \n",
    "\n",
    "**Для Димы: мб как-то переписать более научно?..** \n",
    "\n",
    "1. Генерируем пароль `tor --hash-password mypassword` \n",
    "2. Открываем torrc файл в редакторе вроде vim или [atom](https://atom.io/) \n",
    "3. Сохраняем пароль в наш torrc файл в строку, которая начинается с HashedControlPassword\n",
    "4. Раскоментируем строку, начинающуюся с HashedControlPassword\n",
    "5. Раскоментируем (если она закоментирована) строку ControlPort 9051\n",
    "6. Сохраним изменения. \n",
    "\n",
    "Запускаем tor в терминале. Линукс: `service tor start`, мак: `tor`.\n",
    "\n",
    "Теперь мы готовы парсить. Подгружаем скрипт в наш блокнот. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TorCrawler import TorCrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём свой краулер, в опциях вводим пароль \n",
    "crawler = TorCrawler(ctrl_pass='mypassword')    \n",
    "crawler.ip # Смотрим на свой ip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем сделать get-запрос по аналогии с тем как делали раньше. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_page = 'http://knowyourmeme.com/memes/doge'\n",
    "\n",
    "response = crawler.get(meme_page, headers={'User-Agent': UserAgent().chrome})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем ответ сразу же в формате bs4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Находим внутри что-нибудь нужное. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = response.find('dd', attrs={'class':'views'})\n",
    "views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По дефолту краулер меняет ip каждые $n$ запросов. За это отвечает параметр `n_requests`. По дэфолту он равен 25. Однако при создании нового краулера, его можно настроить по собственному желанию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler.n_requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более того, ip можно при желании поменять вручную. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler.rotate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler.ip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Для Димы: Каких ещё приколов сюда добавить?** \n",
    "\n",
    "В принципе, весь код выше можно перевести на рельсы этого небольшого скрипта, заменив все реквесты на торовские. \n",
    "\n",
    "Победа порвала наши оковы. Великая Сила освободла нас.\n",
    "\n",
    "## Почиташки \n",
    "\n",
    "* [Годная книга](https://github.com/FUlyankin/Parsers/blob/master/Ryan_Mitchell_Web_Scraping_with_Python-_Collecting_Data_from_the_Modern_Web_2015.pdf) про парсинг на английском языке. \n",
    "* [Неплохая инструкция](https://jarroba.com/anonymous-scraping-by-tor-network/) о самостоятельном парсинге через Tor без использования чужих готовых классов. \n",
    "* Димин репозиторий с исследованием мемов и другими ништяками.\n",
    "* [Филин репозиторий](https://fulyankin.github.io/Parsers/) с ещё парой хорошо расписаных блокнотов с парсерами. Репозиторий изначально делался как страничка факультатива для студентов. \n",
    "* [Оригинальный кодекс](http://starwars.wikia.com/wiki/Code_of_the_Sith) адепта тёмной стороны силы.\n",
    "\n",
    "**Диме : может заменить кодекс ситов на какие-нибудь скандинавские приколы?** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
